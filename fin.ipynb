{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18869c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, Input, concatenate\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is my data\n",
    "df = pd.read_csv(\"jupyter3.csv\")\n",
    "df = df.drop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83853835",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEMENTED</th>\n",
       "      <th>BILLS</th>\n",
       "      <th>TAXES</th>\n",
       "      <th>SHOPPING</th>\n",
       "      <th>GAMES</th>\n",
       "      <th>STOVE</th>\n",
       "      <th>MEALPREP</th>\n",
       "      <th>PAYATTN</th>\n",
       "      <th>REMDATES</th>\n",
       "      <th>TRAVEL</th>\n",
       "      <th>DECSUB</th>\n",
       "      <th>DECIN</th>\n",
       "      <th>FILE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\\\Users\\\\kaiso\\\\Andi sf\\\\Image data\\\\Non_Dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\\\Users\\\\kaiso\\\\Andi sf\\\\Image data\\\\Non_Dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\\\Users\\\\kaiso\\\\Andi sf\\\\Image data\\\\Non_Dem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      DEMENTED  BILLS  TAXES  SHOPPING  GAMES  STOVE  MEALPREP  PAYATTN  \\\n",
       "3001         1      2      2         8      8      2         8        1   \n",
       "3002         1      0      1         0      0      0         0        0   \n",
       "3003         1      2      2         2      8      0         1        0   \n",
       "\n",
       "      REMDATES  TRAVEL  DECSUB  DECIN  \\\n",
       "3001         2       3       1      1   \n",
       "3002         0       0       1      1   \n",
       "3003         1       1       1      1   \n",
       "\n",
       "                                                   FILE  \n",
       "3001  C:\\\\Users\\\\kaiso\\\\Andi sf\\\\Image data\\\\Non_Dem...  \n",
       "3002  C:\\\\Users\\\\kaiso\\\\Andi sf\\\\Image data\\\\Non_Dem...  \n",
       "3003  C:\\\\Users\\\\kaiso\\\\Andi sf\\\\Image data\\\\Non_Dem...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3000:3003, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca640929",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1420573a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3782"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dict = {row[-1]:row[1:-1] for row in df.to_numpy()}\n",
    "for file_num in image_dict:\n",
    "    img_path = file_num\n",
    "len(image_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbaf07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Generate image data from file paths\n",
    "images = []\n",
    "for img_path in df[\"FILE\"]:\n",
    "    img_path = img_path\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    images.append(img_arr)\n",
    "X = np.array(images)\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "y = tf.keras.utils.to_categorical(df[\"DEMENTED\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "# Define batch size and shuffle train dataset\n",
    "BATCH_SIZE = 32\n",
    "train_ds = train_ds.shuffle(buffer_size=len(X_train))\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Define the regex pattern to identify the keywords in the file name\n",
    "positive_pattern = r\"(^|\\W)demented(\\W|$)\"\n",
    "\n",
    "# Define a function to extract the label from the image path\n",
    "def extract_label(image_path):\n",
    "    # Extract the file name from the image path\n",
    "    file_name = image_path.split(\"/\")[-1]\n",
    "\n",
    "    # Check if the file name contains the positive keyword\n",
    "    if re.search(positive_pattern, file_name):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to preprocess the image data and extract features\n",
    "def preprocess_image(img_path, num_data):\n",
    "    # Load image and convert to array\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_arr = np.expand_dims(img_arr, axis=0)\n",
    "    \n",
    "    # Apply feature extraction model and flatten output\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(name=\"flatten\"),\n",
    "    ])\n",
    "    flatten_output = tf.keras.backend.function(model.input, model.get_layer(\"flatten\").output)\n",
    "    features = flatten_output(img_arr)\n",
    "    features = np.append(image_dict[num_data], features)\n",
    "    # Extract label from image path\n",
    "    label = extract_label(img_path)\n",
    "\n",
    "    return (features, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = {}\n",
    "for img_path in df[\"FILE\"]:\n",
    "    num_value = img_path\n",
    "    features, label = preprocess_image(img_path, num_value)  # Access image_dict using img_path as key\n",
    "    image_data[img_path] = (tuple(features), label)\n",
    "    \n",
    "print(list(image_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_data['C:\\\\\\\\Users\\\\\\\\kaiso\\\\\\\\Andi sf\\\\\\\\Image data\\\\\\\\Non_Demented\\\\\\\\demented (6).jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949038e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dictionry with file paths as the keys\n",
    "image_dict = {row[-1]:row[1:-1] for row in df.to_numpy()}\n",
    "image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee78e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in image_data.items():\n",
    "    image_data[key] = (tuple(value[0]) + (value[1],),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of items per dataframe\n",
    "max_items_per_df = 1000\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Process the dictionary in groups of max_items_per_df items\n",
    "for i in range(0, len(image_data), max_items_per_df):\n",
    "    # Extract a subset of the dictionary\n",
    "    subset = {k: image_data[k] for k in list(image_data.keys())[i:i+max_items_per_df]}\n",
    "    \n",
    "    # Convert the subset to a DataFrame with index as the keys\n",
    "    subset_df = pd.DataFrame.from_dict(subset, orient='index')\n",
    "    \n",
    "    # If the value is a tuple, split it into separate columns\n",
    "    if isinstance(subset_df.iloc[0][0], tuple):\n",
    "        # Modify the tuple so that the last value is first\n",
    "        subset_df[0] = subset_df[0].apply(lambda x: (x[-1],) + tuple(x[:-1]))\n",
    "        \n",
    "        # Create a new DataFrame from the modified tuples\n",
    "        subset_df = pd.DataFrame(subset_df[0].tolist(), index=subset_df.index)\n",
    "        subset_df.columns = [f\"col{i+1}\" for i in range(subset_df.shape[1])]\n",
    "        \n",
    "        # The first column is the class column\n",
    "        class_values = subset_df.iloc[:, 0]\n",
    "        subset_df = subset_df.iloc[:, 1:]\n",
    "        subset_df.insert(0, 'class', class_values)\n",
    "    \n",
    "    # Append the subset DataFrame to the list\n",
    "    dfs.append(subset_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca48c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_new_and_improved_df = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_data['C:\\\\\\\\Users\\\\\\\\kaiso\\\\\\\\Andi sf\\\\\\\\Image data\\\\\\\\Non_Demented\\\\\\\\demented (441).jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c63788",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_new_and_improved_df.iloc[3000:3003, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1882f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_new_and_improved_df.loc[:,\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78389006",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = the_new_and_improved_df[the_new_and_improved_df.columns[0]]\n",
    "X = the_new_and_improved_df.drop(columns=the_new_and_improved_df.columns[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dede7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the target variable to categorical data\n",
    "num_classes = len(y.unique())\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "search_space = hp()\n",
    "search_space.Int('num_epochs', min_value=10, max_value=50, step=5)\n",
    "search_space.Int('early_stopping_patience', min_value=5, max_value=20, step=5)\n",
    "# Define the search space\n",
    "#search_space = {\n",
    "    #'num_epochs': best_hps.Int(name='num_epochs', min_value=10, max_value=50, step=5),\n",
    "    #'early_stopping_patience': best_hps.Int(name='early_stopping_patience', min_value=5, max_value=20, step=5)\n",
    "#}\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=256, step=32), activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.2)))\n",
    "    model.add(Dense(units=hp.Int('units_2', min_value=16, max_value=64, step=16), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.2)))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1(0.2)))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=hp.Choice('learning_rate', values=[0.001, 0.0001]))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    # Add early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=hp.Int('early_stopping_patience', min_value=5, max_value=20, step=5))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#search_space = hp.Hyperparameter()\n",
    "#search_space.add('num_epochs', hp.Int(min_value=10, max_value=50, step=5))\n",
    "#search_space.add('early_stopping_patience', hp.Int(min_value=5, max_value=20, step=5))\n",
    "##search_space = hp.HyperParameters()\n",
    "#search_space.add('num_epochs', hp.Int(min_value=10, max_value=50, step=5))\n",
    "#search_space.add('early_stopping_patience', hp.Int(min_value=5, max_value=20, step=5))\n",
    "\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=4,\n",
    "    directory='hyperparameters',\n",
    "    project_name='neural_network',\n",
    "    hyperparameters=search_space\n",
    ")\n",
    "\n",
    "best_trials = tuner.oracle.get_best_trials(num_trials=1)\n",
    "if best_trials:\n",
    "    best_hps = best_trials[0].hyperparameters\n",
    "    epochs = best_hps.get('num_epochs')\n",
    "    early_stopping_patience = best_hps.get('early_stopping_patience')\n",
    "else:\n",
    "    epochs = 40\n",
    "    early_stopping_patience = 15\n",
    "\n",
    "tuner.search(\n",
    "    x=X_train_scaled,\n",
    "    y=y_train_categorical,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test_scaled, y_test_categorical),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping_patience)]\n",
    ")\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Units: {}\".format(best_hps.get('units')))\n",
    "print(\"Dropout 1: {}\".format(best_hps.get('dropout_1')))\n",
    "print(\"Units 1: {}\".format(best_hps.get('units_1')))\n",
    "print(\"Units 2: {}\".format(best_hps.get('units_2')))\n",
    "print(\"Dropout 2: {}\".format(best_hps.get('dropout_2')))\n",
    "print(\"Learning rate: {}\".format(best_hps.get('learning_rate')))\n",
    "print(\"Number of epochs: {}\".format(best_hps.get('num_epochs')))\n",
    "print(\"Early stopping patience: {}\".format(best_hps.get('early_stopping_patience')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71593936",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Units: {}\".format(best_hps.get('units')))\n",
    "print(\"Dropout 1: {}\".format(best_hps.get('dropout_1')))\n",
    "print(\"Units 1: {}\".format(best_hps.get('units_1')))\n",
    "print(\"Units 2: {}\".format(best_hps.get('units_2')))\n",
    "print(\"Dropout 2: {}\".format(best_hps.get('dropout_2')))\n",
    "print(\"Learning rate: {}\".format(best_hps.get('learning_rate')))\n",
    "print(\"Number of epochs: {}\".format(best_hps.get('num_epochs')))\n",
    "print(\"Early stopping patience: {}\".format(best_hps.get('early_stopping_patience')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('hyperparameters/neural_network')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fde381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the target variable to categorical data\n",
    "num_classes = len(y.unique())\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "# Create a Keras model with L2 regularization\n",
    "model = Sequential()\n",
    "model.add(Dense(160, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=regularizers.l2(0.2)))\n",
    "model.add(Dropout(0.30000000000000000004))\n",
    "model.add(Dense(96, activation='relu', kernel_regularizer=regularizers.l2(0.2)))\n",
    "model.add(Dense(48, activation='relu', kernel_regularizer=regularizers.l2(0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l1(0.2)))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "from keras import callbacks\n",
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15, restore_best_weights=True, verbose=1)\n",
    "#callbacks = [earlystopping]\n",
    "hist = model.fit(X_train_scaled, y_train_categorical, epochs=20, batch_size=32, validation_data=(X_test_scaled, y_test_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c382edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fin.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c156c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color=\"green\", label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color=\"blue\", label=\"val_accuracy\")\n",
    "plt.plot(hist.history['loss'], color=\"teal\", label='loss')\n",
    "plt.plot(hist.history['val_loss'], color=\"orange\", label=\"val_loss\")\n",
    "fig.suptitle('Training performance with regu (stronger)', fontsize=15)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc= 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test_categorical)\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pepper = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(name='flat'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureX = tf.keras.backend.function(model_pepper.input, model_pepper.get_layer('flat').output)\n",
    "##################image input#####################\n",
    "in_path = 'C:\\\\Users\\\\kaiso\\\\Andi sf\\\\Image data\\\\Non_Demented\\\\non_6.jpg'#<-- image data goes here\n",
    "imgIN = tf.keras.preprocessing.image.load_img(in_path, target_size=(224, 224))\n",
    "imgIN = tf.keras.preprocessing.image.img_to_array(imgIN)\n",
    "imgIN = np.expand_dims(imgIN, axis=0)\n",
    "#############numerical input######################\n",
    "numIN = np.array([0,0,0,0,0,0,0,0,0,0,0])#<--numerical data goes here\n",
    "\n",
    "result = featureX(imgIN)\n",
    "result = result.flatten() \n",
    "result = np.concatenate((result, numIN))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80261844",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b6ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee932926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('C:\\Users\\kaiso\\Andi sf\\fin.ipynb')\n",
    "yhat = model.predict(user_df)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf6b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
